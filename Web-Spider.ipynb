{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Rank - Spidering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import all libraries to be used in our code\n",
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no file spider.sqlite, this creates new one\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x8af4fd4a40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three tables in your database: Pages, Links & Webs \n",
    "# in case there already exists these tables , ignore this \n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, \n",
    "    url TEXT UNIQUE, \n",
    "    html TEXT,\n",
    "    error INTEGER, \n",
    "    old_rank REAL, \n",
    "    new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, \n",
    "    to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs \n",
    "    (url TEXT UNIQUE)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: http://python-data.dr-chuck.net\n"
     ]
    }
   ],
   "source": [
    "# if we already ran the crawl, we will restart it and update our database\n",
    "# if we want to crawl different website, then we need to delete our old database and run this code again and add new web url..\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://python-data.dr-chuck.net']\n"
     ]
    }
   ],
   "source": [
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages:1\n",
      "1 http://python-data.dr-chuck.net (1394) 6\n",
      "How many pages:2\n",
      "4 http://python-data.dr-chuck.net/comments_42.html (3521) 0\n",
      "6 http://python-data.dr-chuck.net/comments_42.json Ignore non text/html page\n",
      "How many pages:5\n",
      "2 http://python-data.dr-chuck.net/geojson Ignore non text/html page\n",
      "5 http://python-data.dr-chuck.net/comments_42.xml Ignore non text/html page\n",
      "7 http://python-data.dr-chuck.net/known_by_42.html (12021) 100\n",
      "105 http://python-data.dr-chuck.net/known_by_Tammara.html (12019) 100\n",
      "62 http://python-data.dr-chuck.net/known_by_Daksh.html (12108) 100\n",
      "How many pages:30\n",
      "164 http://python-data.dr-chuck.net/known_by_Mercy.html (12047) 100\n",
      "290 http://python-data.dr-chuck.net/known_by_Enis.html (12025) 100\n",
      "101 http://python-data.dr-chuck.net/known_by_Khalan.html (12032) 100\n",
      "510 http://python-data.dr-chuck.net/known_by_Murray.html (12053) 100\n",
      "31 http://python-data.dr-chuck.net/known_by_Todd.html (12016) 100\n",
      "396 http://python-data.dr-chuck.net/known_by_Tokunbo.html (12071) 100\n",
      "813 http://python-data.dr-chuck.net/known_by_Jaye.html (12055) 100\n",
      "194 http://python-data.dr-chuck.net/known_by_Tanay.html (12055) 100\n",
      "834 http://python-data.dr-chuck.net/known_by_Choco.html (12072) 100\n",
      "621 http://python-data.dr-chuck.net/known_by_Deonaid.html (12075) 100\n",
      "1106 http://python-data.dr-chuck.net/known_by_Lotte.html (12076) 100\n",
      "821 http://python-data.dr-chuck.net/known_by_Kari.html (12099) 100\n",
      "674 http://python-data.dr-chuck.net/known_by_Viki.html (11990) 100\n",
      "1373 http://python-data.dr-chuck.net/known_by_Fia.html (12061) 100\n",
      "978 http://python-data.dr-chuck.net/known_by_Oluwatoni.html (12061) 100\n",
      "301 http://python-data.dr-chuck.net/known_by_Tiree.html (12028) 100\n",
      "949 http://python-data.dr-chuck.net/known_by_Fergie.html (12066) 100\n",
      "511 http://python-data.dr-chuck.net/known_by_Alieshah.html (12068) 100\n",
      "379 http://python-data.dr-chuck.net/known_by_Sairah.html (12073) 100\n",
      "1546 http://python-data.dr-chuck.net/known_by_Nikita.html (12048) 100\n",
      "1102 http://python-data.dr-chuck.net/known_by_Bill.html (12041) 100\n",
      "1524 http://python-data.dr-chuck.net/known_by_Tyrnan.html (12034) 100\n",
      "1855 http://python-data.dr-chuck.net/known_by_Marwad.html (12066) 100\n",
      "977 http://python-data.dr-chuck.net/known_by_Eshaal.html (12033) 100\n",
      "165 http://python-data.dr-chuck.net/known_by_Samy.html (12062) 100\n",
      "2069 http://python-data.dr-chuck.net/known_by_Azeem.html (12048) 100\n",
      "1613 http://python-data.dr-chuck.net/known_by_Kaine.html (12094) 100\n",
      "1308 http://python-data.dr-chuck.net/known_by_Ina.html (11998) 100\n",
      "485 http://python-data.dr-chuck.net/known_by_Jennie.html (12025) 100\n",
      "1251 http://python-data.dr-chuck.net/known_by_Billie.html (12001) 100\n",
      "How many pages:30\n",
      "1326 http://python-data.dr-chuck.net/known_by_Maira.html (11999) 100\n",
      "728 http://python-data.dr-chuck.net/known_by_Colette.html (12083) 100\n",
      "1247 http://python-data.dr-chuck.net/known_by_Francesca.html (12068) 100\n",
      "1084 http://python-data.dr-chuck.net/known_by_Ada.html (12021) 100\n",
      "1573 http://python-data.dr-chuck.net/known_by_Kayleigh.html (12074) 100\n",
      "2214 http://python-data.dr-chuck.net/known_by_Alaa.html (12096) 100\n",
      "1698 http://python-data.dr-chuck.net/known_by_Ibraheem.html (11994) 100\n",
      "712 http://python-data.dr-chuck.net/known_by_Leya.html (12039) 100\n",
      "661 http://python-data.dr-chuck.net/known_by_Naia.html (12034) 100\n",
      "1111 http://python-data.dr-chuck.net/known_by_Madisin.html (12066) 100\n",
      "313 http://python-data.dr-chuck.net/known_by_Harleen.html (12048) 100\n",
      "1877 http://python-data.dr-chuck.net/known_by_Aled.html (12048) 100\n",
      "319 http://python-data.dr-chuck.net/known_by_Efan.html (12036) 100\n",
      "2428 http://python-data.dr-chuck.net/known_by_Clark.html (12018) 100\n",
      "2332 http://python-data.dr-chuck.net/known_by_Connell.html (12011) 100\n",
      "802 http://python-data.dr-chuck.net/known_by_Alexi.html (12020) 100\n",
      "60 http://python-data.dr-chuck.net/known_by_Fruin.html (12045) 100\n",
      "3124 http://python-data.dr-chuck.net/known_by_Kyna.html (12058) 100\n",
      "3016 http://python-data.dr-chuck.net/known_by_Sophia.html (11956) 100\n",
      "2779 http://python-data.dr-chuck.net/known_by_Kade.html (11987) 100\n",
      "49 http://python-data.dr-chuck.net/known_by_Bjorn.html (12020) 100\n",
      "2526 http://python-data.dr-chuck.net/known_by_Cobie.html (12000) 100\n",
      "3254 http://python-data.dr-chuck.net/known_by_Jeya.html (12065) 100\n",
      "494 http://python-data.dr-chuck.net/known_by_Kyron.html (12021) 100\n",
      "3095 http://python-data.dr-chuck.net/known_by_Olivier.html (12043) 100\n",
      "2044 http://python-data.dr-chuck.net/known_by_Saschamarie.html (12111) 100\n",
      "1957 http://python-data.dr-chuck.net/known_by_Sabrine.html (12029) 100\n",
      "59 http://python-data.dr-chuck.net/known_by_Kahlia.html (12035) 100\n",
      "3494 http://python-data.dr-chuck.net/known_by_Kerry.html (12027) 100\n",
      "2613 http://python-data.dr-chuck.net/known_by_Avsta.html (12073) 100\n",
      "How many pages:20\n",
      "2655 http://python-data.dr-chuck.net/known_by_Zena.html (12054) 100\n",
      "1384 http://python-data.dr-chuck.net/known_by_Sukhpreet.html (12031) 100\n",
      "2201 http://python-data.dr-chuck.net/known_by_Alighia.html (12053) 100\n",
      "1369 http://python-data.dr-chuck.net/known_by_Abdihakim.html (12033) 100\n",
      "3022 http://python-data.dr-chuck.net/known_by_Kerrieann.html (12097) 100\n",
      "763 http://python-data.dr-chuck.net/known_by_Nana.html (12000) 100\n",
      "3169 http://python-data.dr-chuck.net/known_by_Gurdeep.html (12044) 100\n",
      "218 http://python-data.dr-chuck.net/known_by_Laiyah.html (12051) 100\n",
      "2361 http://python-data.dr-chuck.net/known_by_Tighan.html (12083) 100\n",
      "2673 http://python-data.dr-chuck.net/known_by_James.html (12049) 100\n",
      "2972 http://python-data.dr-chuck.net/known_by_Khadijah.html (12096) 100\n",
      "272 http://python-data.dr-chuck.net/known_by_Avani.html (12026) 100\n",
      "950 http://python-data.dr-chuck.net/known_by_Peregrine.html (12060) 100\n",
      "2565 http://python-data.dr-chuck.net/known_by_Mariam.html (12088) 100\n",
      "2400 http://python-data.dr-chuck.net/known_by_Kelis.html (12043) 100\n",
      "604 http://python-data.dr-chuck.net/known_by_Alistair.html (12023) 100\n",
      "1901 http://python-data.dr-chuck.net/known_by_Bret.html (12069) 100\n",
      "1237 http://python-data.dr-chuck.net/known_by_Data.html (12094) 100\n",
      "2300 http://python-data.dr-chuck.net/known_by_Meryem.html (12032) 100\n",
      "548 http://python-data.dr-chuck.net/known_by_Karla.html (12061) 100\n",
      "How many pages:\n"
     ]
    }
   ],
   "source": [
    "# this asks you to enter how many unretrieved pages you want to crawl and enter the new links found into pages table \n",
    "# for each page it prints out the number of new links found for that page\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "        # Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
